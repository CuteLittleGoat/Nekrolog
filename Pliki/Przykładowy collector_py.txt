import json
import re
import sys
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

import requests
from bs4 import BeautifulSoup
from dateutil.parser import parse as dtparse

HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; Kapelanka-ObitsBot/1.0; +local-script)"
}
TIMEOUT = 25

TARGET_PHRASES = [
    "Helena Gawin",
    "Helena Dereń",
    "Helena Dereń-Gawin",
    "Helena Dereń Gawin",
    "Helena Gawin-Dereń",
    "Helena Gawin Dereń",
]

def norm(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "").strip()).lower()

def safe_date(s: str) -> Optional[str]:
    s = (s or "").strip()
    if not s:
        return None
    try:
        # accept YYYY-MM-DD, DD.MM.YYYY etc.
        d = dtparse(s, dayfirst=True, yearfirst=False)
        return d.date().isoformat()
    except Exception:
        return None

@dataclass
class Row:
    kind: str  # "death" | "funeral"
    name: str
    date_death: Optional[str] = None
    date_funeral: Optional[str] = None
    time_funeral: Optional[str] = None
    place: Optional[str] = None
    source_id: Optional[str] = None
    source_name: Optional[str] = None
    url: Optional[str] = None
    note: Optional[str] = None

def fetch_html(url: str) -> BeautifulSoup:
    r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
    r.raise_for_status()
    return BeautifulSoup(r.text, "lxml")

# -------- Parsers --------

def parse_zck_funerals(source: Dict[str, Any]) -> Tuple[List[Row], List[Row]]:
    """
    ZCK /funerals zawiera listy per cmentarz: godzina, miejsce (kaplica/sala), imię nazwisko (lat X)
    """
    soup = fetch_html(source["url"])
    rows_fun: List[Row] = []

    # Na stronie są nagłówki typu "#### Cmentarz XYZ" i pod nimi linie z godzinami.
    # W HTML bywa to jako h4/h3 + kolejne teksty / listy.
    current_cemetery = None

    for tag in soup.find_all(["h4", "h3", "p", "div", "li"]):
        txt = tag.get_text(" ", strip=True)
        if not txt:
            continue
        if txt.lower().startswith("cmentarz "):
            current_cemetery = txt
            continue

        # Przykład: "10:20 Kaplica Małgorzata Wieczorek (lat 72)"
        m = re.match(r"^(\d{1,2}:\d{2})\s+(.+?)\s+([A-ZĄĆĘŁŃÓŚŹŻ].+?)\s*\(lat\s*(\d+)\)\s*$", txt)
        if m and current_cemetery:
            time_ = m.group(1)
            where_ = m.group(2)
            name_ = m.group(3)
            age_ = m.group(4)
            rows_fun.append(Row(
                kind="funeral",
                name=name_,
                time_funeral=time_,
                place=f"{current_cemetery} — {where_}",
                source_id=source["id"],
                source_name=source["name"],
                url=source["url"],
                note=f"wiek: {age_}"
            ))

    return [], rows_fun

def parse_podwawelskie_nekrologi(source: Dict[str, Any]) -> Tuple[List[Row], List[Row]]:
    """
    Tabela/układ listy: Imię, Nazwisko, data ur., data zm.
    """
    soup = fetch_html(source["url"])
    deaths: List[Row] = []

    # Strona jest paginowana (str=...), ale na start bierzemy pierwszą stronę.
    # Rozpoznajemy rekordy po sekwencji pól zawierających daty.
    text = soup.get_text("\n", strip=True)
    # Heurystyka: linie z datą YYYY-MM-DD
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]

    # Przechodzimy oknami po liniach: Imię / Nazwisko / data_ur / data_zm
    for i in range(len(lines) - 3):
        if re.match(r"^\d{4}-\d{2}-\d{2}$", lines[i+2]) and re.match(r"^\d{4}-\d{2}-\d{2}$", lines[i+3]):
            name = f"{lines[i]} {lines[i+1]}".strip()
            b = lines[i+2]
            d = lines[i+3]
            deaths.append(Row(
                kind="death",
                name=name,
                date_death=safe_date(d) or d,
                place="Parafia MB Fatimskiej (Podwawelskie) — Nekrologi",
                source_id=source["id"],
                source_name=source["name"],
                url=source["url"],
                note=f"ur.: {b}"
            ))

    # Dedup (czasem heurystyka złapie duble)
    uniq = {}
    for r in deaths:
        k = (norm(r.name), r.date_death, r.source_id)
        uniq[k] = r
    return list(uniq.values()), []

def parse_podwawelskie_ogloszenia(source: Dict[str, Any]) -> Tuple[List[Row], List[Row]]:
    """
    Wyciąga wzmianki typu: 'W minionym tygodniu pożegnaliśmy: + ...'
    """
    soup = fetch_html(source["url"])
    txt = soup.get_text(" ", strip=True)
    deaths: List[Row] = []

    m = re.search(r"W minionym tygodniu pożegnaliśmy:(.+?)(Dobry Jezu|Pomódlmy|$)", txt, flags=re.IGNORECASE)
    if m:
        blob = m.group(1)
        # rozdzielamy po przecinkach i plusach
        candidates = re.split(r"[,+]\s*\+?\s*", blob)
        for c in candidates:
            c = c.strip(" .;:–-")
            if len(c) < 3:
                continue
            # weź tylko coś co wygląda na imię nazwisko
            if re.search(r"[A-ZĄĆĘŁŃÓŚŹŻ][a-ząćęłńóśźż]+\s+[A-ZĄĆĘŁŃÓŚŹŻ][a-ząćęłńóśźż-]+", c):
                deaths.append(Row(
                    kind="death",
                    name=c,
                    place="Parafia MB Fatimskiej — ogłoszenia",
                    source_id=source["id"],
                    source_name=source["name"],
                    url=source["url"]
                ))

    return deaths, []

def parse_intencje_simple(source: Dict[str, Any]) -> Tuple[List[Row], List[Row]]:
    """
    Intencje: linie typu '7.00 +Jan Kowalski' — traktujemy jako sygnał zgonu (soft).
    """
    soup = fetch_html(source["url"])
    txt = soup.get_text("\n", strip=True)
    deaths: List[Row] = []

    for line in txt.splitlines():
        line = line.strip()
        # przykłady: "7.00 +Jan, Helena i Elżbieta Rusnak" (czasem wiele osób)
        m = re.search(r"(\+|\u2020)\s*([A-ZĄĆĘŁŃÓŚŹŻ].{3,80})$", line)
        if not m:
            continue
        tail = m.group(2)
        # odfiltruj oczywiste "za parafian", "za zmarłych..." bez nazwisk
        if re.search(r"za parafian|za zmar", tail, flags=re.IGNORECASE):
            continue
        # bierzemy tylko fragmenty z literami; bez przesady z długością
        tail = re.sub(r"\s+", " ", tail).strip()
        if len(tail) > 90:
            continue

        deaths.append(Row(
            kind="death",
            name=tail,
            place="Intencje mszalne (wzmianka '+')",
            source_id=source["id"],
            source_name=source["name"],
            url=source["url"],
            note="Wpis z intencji mszalnych (traktuj jako wskazówkę, nie oficjalny nekrolog)."
        ))

    # dedup
    uniq = {}
    for r in deaths:
        k = (norm(r.name), r.source_id)
        uniq[k] = r
    return list(uniq.values()), []

def parse_intencje_debniki(source: Dict[str, Any]) -> Tuple[List[Row], List[Row]]:
    """
    Dębniki: /intencje/ to archiwum (wiele wpisów). Bierzemy pierwszą stronę.
    """
    soup = fetch_html(source["url"])
    deaths: List[Row] = []

    # Pobierz tekst z fragmentów, gdzie występuje "+ "
    for p in soup.find_all(["p", "li", "div"]):
        t = p.get_text(" ", strip=True)
        if "+" not in t and "†" not in t:
            continue
        # wyciągnij po plusie
        for m in re.finditer(r"(\+|†)\s*([A-ZĄĆĘŁŃÓŚŹŻ][^•]{3,80})", t):
            name = re.sub(r"\s+", " ", m.group(2)).strip(" .;:")
            if re.search(r"za zmar|za paraf", name, flags=re.IGNORECASE):
                continue
            deaths.append(Row(
                kind="death",
                name=name,
                place="Dębniki — intencje (wzmianka '+/†')",
                source_id=source["id"],
                source_name=source["name"],
                url=source["url"],
                note="Wpis z intencji mszalnych (wskazówka)."
            ))

    uniq = {}
    for r in deaths:
        k = (norm(r.name), r.source_id)
        uniq[k] = r
    return list(uniq.values()), []

def parse_intencje_pychowice(source: Dict[str, Any]) -> Tuple[List[Row], List[Row]]:
    """
    Pychowice: podobnie jak proste intencje.
    """
    soup = fetch_html(source["url"])
    txt = soup.get_text("\n", strip=True)
    deaths: List[Row] = []
    for line in txt.splitlines():
        line = line.strip()
        m = re.search(r"(\+|†)\s*([A-ZĄĆĘŁŃÓŚŹŻ].{3,80})$", line)
        if not m:
            continue
        tail = re.sub(r"\s+", " ", m.group(2)).strip()
        if re.search(r"za parafian|za zmar", tail, flags=re.IGNORECASE):
            continue
        deaths.append(Row(
            kind="death",
            name=tail,
            place="Pychowice — intencje (wzmianka '+/†')",
            source_id=source["id"],
            source_name=source["name"],
            url=source["url"],
            note="Wpis z intencji mszalnych (wskazówka)."
        ))

    uniq = {}
    for r in deaths:
        k = (norm(r.name), r.source_id)
        uniq[k] = r
    return list(uniq.values()), []

def parse_grobonet_nekrologi(source: Dict[str, Any]) -> Tuple[List[Row], List[Row]]:
    """
    Grobonet: layout bywa różny. Robimy heurystykę:
    - szukamy w tabelach/listach elementów z datą i nazwiskiem
    """
    soup = fetch_html(source["url"])
    deaths: List[Row] = []

    # często rekordy są w tabeli
    for tr in soup.find_all("tr"):
        tds = [td.get_text(" ", strip=True) for td in tr.find_all(["td", "th"])]
        if not tds:
            continue
        rowtxt = " | ".join(tds)
        # spróbuj wyłuskać imię+nazwisko oraz datę
        m_name = re.search(r"([A-ZĄĆĘŁŃÓŚŹŻ][a-ząćęłńóśźż-]+\s+[A-ZĄĆĘŁŃÓŚŹŻ][a-ząćęłńóśźż-]+)", rowtxt)
        m_date = re.search(r"(\d{4}-\d{2}-\d{2}|\d{2}\.\d{2}\.\d{4})", rowtxt)
        if m_name and m_date:
            deaths.append(Row(
                kind="death",
                name=m_name.group(1),
                date_death=safe_date(m_date.group(1)),
                place="Cmentarz Salwatorski (Grobonet) — Nekrologi",
                source_id=source["id"],
                source_name=source["name"],
                url=source["url"]
            ))

    uniq = {}
    for r in deaths:
        k = (norm(r.name), r.date_death, r.source_id)
        uniq[k] = r
    return list(uniq.values()), []

def parse_funeral_home_nekrologi_generic(source: Dict[str, Any]) -> Tuple[List[Row], List[Row]]:
    """
    Karawan/Gabriel mają publiczne listy nekrologów. Robimy parser ogólny:
    - zbieramy linki, w których występuje słowo 'nekrolog' lub wyglądają jak wpisy,
      i próbujemy wyciągnąć: imię nazwisko, datę zgonu, datę pogrzebu.
    """
    soup = fetch_html(source["url"])
    base_url = source["url"]
    funerals: List[Row] = []
    deaths: List[Row] = []

    for a in soup.find_all("a", href=True):
        href = a["href"].strip()
        text = a.get_text(" ", strip=True)
        if not text:
            continue
        # filtr linków
        if "nekrolog" not in href and "nekrolog" not in text.lower():
            continue
        if href.startswith("/"):
            url = re.match(r"^https?://[^/]+", base_url).group(0) + href
        elif href.startswith("http"):
            url = href
        else:
            url = base_url.rstrip("/") + "/" + href.lstrip("/")

        # spróbuj rozpoznać nazwę osoby
        m_name = re.search(r"([A-ZĄĆĘŁŃÓŚŹŻ][a-ząćęłńóśźż-]+(?:\s+[A-ZĄĆĘŁŃÓŚŹŻ][a-ząćęłńóśźż-]+)+)", text)
        if not m_name:
            continue

        # pobierz szczegóły z podstrony (ostrożnie, max kilka)
        try:
            detail = fetch_html(url)
        except Exception:
            continue

        page = detail.get_text(" ", strip=True)

        # data zgonu / data pogrzebu
        dd = None
        df = None
        tf = None

        m_dd = re.search(r"(zm\.\s*|data\s+zgonu[:\s]*)(\d{2}\.\d{2}\.\d{4}|\d{4}-\d{2}-\d{2})", page, flags=re.IGNORECASE)
        if m_dd:
            dd = safe_date(m_dd.group(2)) or m_dd.group(2)

        m_df = re.search(r"(pogrzeb\s*(dziś|jutro|w\s+dniu)?[:\s]*|data\s+pogrzebu[:\s]*)(\d{2}\.\d{2}\.\d{4}|\d{4}-\d{2}-\d{2})", page, flags=re.IGNORECASE)
        if m_df:
            df = safe_date(m_df.group(3)) or m_df.group(3)

        m_tf = re.search(r"(msza|godz\.|o\s*godz\.|godzina)[:\s]*([0-2]?\d[:.][0-5]\d)", page, flags=re.IGNORECASE)
        if m_tf:
            tf = m_tf.group(2).replace(".", ":")

        name = m_name.group(1).strip()

        if dd:
            deaths.append(Row(
                kind="death",
                name=name,
                date_death=dd,
                place="Dom pogrzebowy — nekrolog",
                source_id=source["id"],
                source_name=source["name"],
                url=url
            ))
        if df or tf:
            funerals.append(Row(
                kind="funeral",
                name=name,
                date_funeral=df,
                time_funeral=tf,
                place="Dom pogrzebowy — nekrolog (szczegóły)",
                source_id=source["id"],
                source_name=source["name"],
                url=url
            ))

    # dedup
    def dedup(rows):
        uniq = {}
        for r in rows:
            k = (norm(r.name), r.date_death, r.date_funeral, r.time_funeral, r.source_id)
            uniq[k] = r
        return list(uniq.values())

    return dedup(deaths), dedup(funerals)

PARSERS = {
    "zck_funerals": parse_zck_funerals,
    "table_nekrologi_podwawelskie": parse_podwawelskie_nekrologi,
    "podwawelskie_ogloszenia": parse_podwawelskie_ogloszenia,
    "intencje_simple": parse_intencje_simple,
    "intencje_debniki": parse_intencje_debniki,
    "intencje_pychowice": parse_intencje_pychowice,
    "grobonet_nekrologi": parse_grobonet_nekrologi,
    "funeral_home_nekrologi_generic": parse_funeral_home_nekrologi_generic,
    # "facebook_optional": ... (celowo pominięte)
}

def main(out_path: str = "data/latest.json", sources_path: str = "sources.json") -> int:
    with open(sources_path, "r", encoding="utf-8") as f:
        cfg = json.load(f)

    all_deaths: List[Row] = []
    all_funerals: List[Row] = []

    for s in cfg["sources"]:
        ptype = s["type"]
        parser = PARSERS.get(ptype)
        if not parser:
            print(f"[WARN] No parser for type={ptype} source={s['id']}", file=sys.stderr)
            continue

        try:
            deaths, funs = parser(s)
            all_deaths.extend(deaths)
            all_funerals.extend(funs)
            print(f"[OK] {s['id']} deaths={len(deaths)} funerals={len(funs)}")
        except Exception as e:
            print(f"[ERR] {s['id']} => {e}", file=sys.stderr)

    # sort: najnowsze na górze
    def sort_key_death(r: Row):
        # brak daty => na dół
        return r.date_death or "0000-00-00"

    def sort_key_fun(r: Row):
        return (r.date_funeral or "0000-00-00", r.time_funeral or "00:00")

    all_deaths.sort(key=sort_key_death, reverse=True)
    all_funerals.sort(key=sort_key_fun, reverse=True)

    payload = {
        "generated_at": datetime.utcnow().isoformat() + "Z",
        "sources": [{"id": s["id"], "name": s["name"], "url": s["url"], "type": s["type"]} for s in cfg["sources"]],
        "recent_deaths": [asdict(r) for r in all_deaths[:200]],
        "upcoming_funerals": [asdict(r) for r in all_funerals[:200]],
        "target_phrases": TARGET_PHRASES
    }

    import os
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    # szybki log: czy znaleziono Helenę
    hits = []
    for r in all_deaths + all_funerals:
        hay = norm(f"{r.name} {r.note or ''} {r.place or ''}")
        if any(norm(p) in hay for p in TARGET_PHRASES):
            hits.append(r)
    print(f"[INFO] Helena-hits: {len(hits)}")

    return 0

if __name__ == "__main__":
    out = sys.argv[1] if len(sys.argv) > 1 else "data/latest.json"
    sys.exit(main(out_path=out))
